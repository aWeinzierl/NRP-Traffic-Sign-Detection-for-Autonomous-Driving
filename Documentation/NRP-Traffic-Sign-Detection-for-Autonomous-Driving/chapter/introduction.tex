
\section{Introduction and motivation}
Traffic sign recognition is one of the features of Advanced driver-assistance systems (ADAS) and also an inevitable problem for autonomous driving. 

Traffic signs are usually placed by the roadside or above the road, providing information or giving guidance to road users for a safer driving. Traffics signs contains important information required for driving, such as directing drivers to drive in the correct lane at proper speed or warning drivers of obstacles and potential risks. 

Forward-facing cameras mounted in vehicles is usually required to detect and recognize road signs. There are various algorithms for traffic sign recognition, such as detection based on the shape of the sign board, character recognition and deep learning methods.  Nowadays, convolutional neural networks (CNN) have been used to locate and classify a large variety of traffic signs, mainly driven by the requirements of autonomous cars. The neural network will first be trained with large amount of images of the to be detected traffic signs and their corresponding "tags". The trained network can then work in real-time to detect traffic signs captured by the camera in vehicles.

In this project, a small dataset of three traffic signs (two speed limit signs and a stop sign) is used to train a pre-trained convolutional neural network. An analysis of the recognition accuracy of the neural network will be given in this report. Apart from that, a street scene with the three different traffic signs is built in Neurorobotics Platform (NRP \cite{nrp}) and the Husky robot with a forward-facing camera is used as the car model. The real-time camera view of this robot will be fed in our trained CNN and the robot will response to the recognition result with a look-up-table. Thanks to NRP, where simulations are calculated by the physics engine, the dynamic performance of the car incorporating the traffic-sign recognition network will be analyzed in this work.
